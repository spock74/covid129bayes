---
title: "Inferencia. Fase I"
author: "José Elvano Moraes"
date: "17/04/2021"
output:
  html_document:
    df_print: paged
  pdf_document: 
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries, echo=FALSE}
suppressPackageStartupMessages(expr = library(tidyverse))
suppressPackageStartupMessages(expr = library(bnlearn))
suppressPackageStartupMessages(expr = library(bnstruct))
suppressPackageStartupMessages(expr = library(readr))
suppressPackageStartupMessages(expr = library(Rgraphviz))
suppressPackageStartupMessages(expr = library(gRain))
suppressPackageStartupMessages(expr = library(Rmpfr))
```

# Redes Bayesianas
Probabilistic reasoning on BNs works in the framework of Bayesian statistics and focuses on the computation of posterior probabilities or densities.
For example, suppose we have learned a BN B with DAG G and parameters Θ. We want to use B to investigate the effects of a new piece of evidence E using the knowledge encoded in B, that is, to investigate the posterior distribution

$$P(\mathbf{X} | \mathbf{E},\mathbf{\mathcal{B}})=P(\mathbf{X} | \mathbf{E},\mathbf{G},\Theta)$$

The first step of fitting a Bayesian network is called structure learning and consists in identifying the graph structure of the Bayesian network. Ideally, it should be the minimal I-map of the dependence structure of the data or, failing that, it should at least result in a dis- tribution as close as possible to the correct one in the probability space. Several algorithms have been proposed in the literature for structure learning. Despite the variety of theoretical backgrounds and terminology, they fall under three broad cate- gories: constraint-based, score-based, and hybrid algorithms. As an alternative, the network structure can be built manually from the domain knowledge of a human expert and prior information available on the data.

he second step is called parameter learning. As the name suggests, it imple- ments the estimation of the parameters of the global distribution. This task can be performed efficiently by estimating the parameters of the local distributions implied by the structure obtained in the previous step.

Questions that can be asked are called queries and are typically an event of interest. The two most common queries are conditional probability (CPQ) and maximum a posteriori (MAP) queries, also known as most probable explanation (MPE) queries

```{r echo=FALSE}
rm(list=ls())
```


```{r echo=FALSE}
load(file = 'inferencia-01.RData')
#ls()
```
## some content here
```{r}
glimpse(ddf)
```

## some content here
```{r echo=FALSE}
par(mfrow = c(1, 2))
graphviz.plot(bn.no.wl, 
              shape='rectangle', 
              highlight = NULL, 
              main = 'DAG sem WL')
graphviz.plot(bn.wl, 
              shape='rectangle', 
              highlight = list(arcs = wl), 
              main = 'DAG com imposição de uma WL')
```

## some content here
```{r echo=FALSE}
par(mfrow=c(1,2))
graphviz.chart(fitted.no.wl,
               type = "barprob", 
               col = "darkblue",
               bg = "azure", 
               bar.col = "darkblue", 
               main = "DAG sem WL")

graphviz.chart(fitted.wl,  
               type = "barprob", 
               col = "darkblue", 
               bg = "azure", 
               bar.col = "darkblue", 
               main = "DAG com WL")
```

## DAG médio e *força da correlação* entre pares de variáveis
```{r echo=FALSE}
## Obtendo o DAG médio: fase de Bootstrap usando 300 ciclos
options(digits=2)

thr <- paste('Thr: ', attr(str.diff, "threshold"))

strength.plot(avg.diff, 
              str.diff, 
              sub = "A espessura dos arcos representa a correlação de Pearson entre variáveis",
              shape = "rectangle", 
              main = paste("Iter = ", boots.trap, thr))

```


# Descrição das redes
```{r}
avg.diff
avg.simpler
```

## Dag médio *versus* DAG único
```{r echo=FALSE}
par(mfrow = c(1, 2))
graphviz.compare(avg.diff,
                 bn.no.wl,
                 shape = "rectangle",
                 main = c("DAG médio", "DAG único sem WL"))
```

```{r echo=FALSE}
par(mfrow = c(1, 2))
graphviz.compare(avg.diff,
                 bn.wl,
                 shape = "rectangle",
                 main = c("DAG médio", "DAG único com WL"))
```


## DAG médio *versus* simplificado

DAG médio obtido pelo processo de bootstrapping e DAG *simplicada* no qual desenhou-se somente os arcos com *strenght*, (correlação de Pearson) acima de 0.95.

Os nodos coloridos formam o *markov blanket* da variável EVOLUCAO, ou seja, os nodos suficientes para descrevere completamente a distribuição estatistica da variável

```{r echo=FALSE}
mb.diff <-  mb(avg.diff, 'EVOLUCAO')
mb.simpler <- mb(avg.simpler, 'EVOLUCAO')

par(mfrow = c(1, 2))
graphviz.plot(x = avg.diff, 
              shape = 'rectangle', 
              main = "Rede Média",
              highlight = list(nodes=mb.diff, 
                               col = "darkblue", 
                               fill = "lightblue") )
graphviz.plot(x = avg.simpler, 
              shape = 'rectangle', 
              main = "Rede simpificada",
              highlight = list(nodes=mb.simpler, 
                               col = "darkblue", 
                               fill = "lightgreen") )
```

## Performing likelihood weighting 

TODO

With cpquery by setting method = "lw" and specifying the evidence as a named list with one element for each node we are conditioning on 
```{r echo=FALSE}
fitt <- bn.fit(bn.no.wl, amostra.90pc.seed.42)
nparticles = seq(from = 5 * 10^3, to = 10^5, by = 5 * 10^3)
prob = matrix(0, nrow = length(nparticles), ncol = 20)
for (i in seq_along(nparticles))
  for (j in 1:20)
    prob[i, j] = cpquery(fitt, 
                         event = (UTI == "1"),
                         evidence = list(SATURACAO = "2"), 
                         method = "lw",
                         n = nparticles[i])
```


```{r echo=FALSE}
matplot(x=1:20, y=prob[,1:20], 
        pch = 21, 
        col="blue", 
        main="Desempenho de método likelihood weighting", 
        ylab = "Probabilidade Estimada", 
        xlab = "Número de partículas")
```

# Inferência

In practice, probabilistic reasoning on Bayesian networks has its roots embedded in Bayesian statistics and focuses on the computation of posterior probabilities or densities. For example, suppose we have learned a Bayesian network B with.
Bayesian inference on the other hand is often a follow-up to Bayesian net- work learning and deals with inferring the state of a set of variables given the state of others as evidence.

Bayesian networks, like other statistical models, can be used to answer questions about the nature of the data that go beyond the mere description of the observed sample. Techniques used to obtain those answers based on new evidence are known in general as inference. For Bayesian networks, the process of answering these questions is also known as probabilistic reasoning or belief updating, while the questions themselves are called queries.

In practice, probabilistic reasoning on Bayesian networks has its roots embedded in Bayesian statistics and focuses on the computation of posterior probabilities or densities. For example, suppose we have learned a Bayesian network B with

structure G and parameters $\Theta$ , under one of the distributional assumptions detailed in Sect. 2.2.4. Subsequently, we want to investigate the effects of a new piece of evidence E on the distribution of X using the knowledge encoded in B, that is, to investigate the posterior distribution $P(X | E, B) = P(X | E, G, \Theta )$.
The approaches used for this kind of analysis vary depending on the nature of E and on the nature of information we are interested in. The two most common kinds of evidence are as follows:

- Hard evidence, an instantiation of one or more variables in the network. In other words,

- Soft evidence, a new distribution for one or more variables in the network. Since both the network structure and the distributional assumptions are treated as fixed, soft evidence is usually specified as a new set of parameters,

As far as queries are concerned, we will focus on conditional probability queries (CPQ) and maximum a posteriori (MAP) queries, also known as most probable explanation (MPE) queries. Both apply mainly to hard evidence, even though they can be used in combination with soft evidence.

## Predição 

Sabe-se que paciente **está** no CTI, estima-se qual a distribuição marginal de probabilidade das variáveis

- IDADE
- RENAL
- EVOLUCAO
- ANTIVIRAL

**Oberve que as variáveis RENAL é independentes das demais na rede**


-------------


### Legenda para a interpretação das probabilidade das variáveis 

**EVOLUCAO**

1 - Cura, 2 - Obito por COVID-19, 3 - Óbito por outras causas, 9 - Ignorado

**ANTIVIRAL**

1 - Oseltamivir, 2 - Zanamivir, 3 - Outro

**RENAL**

1 - sim, 2 - não, 3 - ignorado


--------------


### Cenário 1

- UTI: não
- SUPORT_VEN: não

```{r echo=FALSE}
#UTI
#  1-Sim 
#  2-Nao 
#  9-Ignorado

#SUPORT_VEN
# 1-Sim, invasivo 
# 2-Sim, nao invasivo 
# 3-Nao
# 9-Ignorado

fitt2 <- bn.fit(avg.simpler, amostra.90pc.seed.42)
junction = compile(as.grain(fitt2))
jedu = setEvidence(propagate = FALSE, junction, node = c("UTI", "SUPORT_VEN"), states = c("2", "3") )
querygrain(jedu, 
           nodes = c("IDADE", "RENAL", "EVOLUCAO", "ANTIVIRAL"),
           type = "marginal")
```
### Cenário 2

- UTI: sim
- SUPORT_VEN: invasivo

```{r echo=FALSE}
#UTI
#  1-Sim 
#  2-Nao 
#  9-Ignorado

#SUPORT_VEN
# 1-Sim, invasivo 
# 2-Sim, nao invasivo 
# 3-Nao
# 9-Ignorado

fitt3 <- bn.fit(avg.simpler, amostra.90pc.seed.42)
junction = compile(as.grain(fitt))
jedu = setEvidence(propagate = FALSE, junction, node = c("UTI", "SUPORT_VEN"), states = c("1", "1") )
querygrain(jedu, 
           nodes = c("IDADE", "RENAL", "EVOLUCAO", "ANTIVIRAL"),
           type = "marginal")
```

### Cenário 3

- UTI: sim
- SUPORT_VEN: nao invasivo

```{r echo=FALSE}
#UTI
#  1-Sim 
#  2-Nao 
#  9-Ignorado

#SUPORT_VEN
# 1-Sim, invasivo 
# 2-Sim, nao invasivo 
# 3-Nao
# 9-Ignorado

fitt3 <- bn.fit(avg.simpler, amostra.90pc.seed.42)
junction = compile(as.grain(fitt))
jedu = setEvidence(propagate = FALSE, junction, node = c("UTI", "SUPORT_VEN"), states = c("1", "2") )
querygrain(jedu, 
           nodes = c("IDADE", "RENAL", "EVOLUCAO", "ANTIVIRAL"),
           type = "marginal")
```

---------------

# Simulação

```{r}
#amostra.simulada = cpdist(fitt3, nodes = c("IDADE", "RENAL", "EVOLUCAO", "ANTIVIRAL"), evidence = UTI == "1")
``` 

```{r}
#ggplot(amostra.simulada, aes(IDADE)) + geom_histogram(stat = "count") + ylim(0, 4000)
```

```{r}
#amostra.simulada = cpdist(fitt3, nodes = c("IDADE", "RENAL", "EVOLUCAO", "ANTIVIRAL"), evidence = UTI == "2")
```

```{r}
#ggplot(amostra.simulada, aes(IDADE)) + geom_histogram(stat = "count") + ylim(0, 4000)
```

```{r}
bn.wl.fit <- bn.fit(bn.wl, amostra.90pc.seed.42)
```


```{r}
nodos <- c("OBESIDADE", "DIABETES", 
           "EVOLUCAO", "CARDIOPATI", "SUPORT_VEN", 
           "IDADE")

nodos.2 <- c("IDADE","FEBRE","GARGANTA","SATURACAO",
             "EVOLUCAO","RENAL","DIABETES","OBESIDADE",
             "PERD_OLFT", "PERD_PALA", "NEUROLOGIC", 
             "PNEUMOPATI", 
             #"UTI", 
             "CARDIOPATI", 
             "SUPORT_VEN", "ASMA","ANTIVIRAL")

amostra.simulada = cpdist(bn.wl.fit, 
                          nodes = nodos, 
                          evidence = (UTI == "2"))
amostra.simulada.2 = cpdist(bn.wl.fit, 
                          nodes = nodos.2, 
                          evidence = (UTI == "2"))

amostra.real <- dplyr::filter(ddf, UTI == 1)  

amostra.real.1 <- sample_n(amostra.real, 
                         size = nrow(amostra.simulada))[,nodos]

amostra.real.2 <- sample_n(amostra.real, 
                           size = nrow(amostra.simulada.2))[,nodos.2]
```


```{r}
amostra.simulada.2 <- na.omit(amostra.simulada.2)
rede.simulada.2 <- mmhc(amostra.simulada.2)
rede.real.2 <- mmhc(amostra.real.2)
```


```{r}
par(mfrow = c(1, 2))
graphviz.chart(bn.fit(rede.simulada.2, amostra.simulada.2),  
               type = "barprob", 
               col = "darkblue", 
               bg = "azure", 
               bar.col = "darkblue", 
               main = "DAG amostra simulada")

graphviz.chart(bn.fit(rede.real.2, amostra.real.2),  
               type = "barprob", 
               col = "darkblue", 
               bg = "azure", 
               bar.col = "darkblue", 
               main = "DAG amostra real")
```
# Trabalhando com cpquery

```{r}
# idadee\; [1,37] (37,73] (73,109]


fitt.1 <- bn.fit(avg.diff, amostra.90pc.seed.42)

prob.uti.1 <- cpquery(fitt.1, 
         evidence = list( 
           #'SATURACAO' = "1",
               #(IDADE == "(37,73]") &
               # (FEBRE == 1) &
               #  (DIABETES == 2) &
               #  (RENAL == 2) &
               "UTI" = "1" #,
               #'SUPORT_VEN' = "1" 
               ),
         event =  (EVOLUCAO == 2) ,
         method = "lw")
prob.uti.1
```

```{r}
x <- amostra.90pc.seed.42 %>% filter(
                                    #SATURACAO == 1, 
                                     UTI == 1#, 
                                     #FEBRE == 1, 
                                     #DIABETES == 2, 
                                     #RENAL == 2,
                                     #IDADE == "(37,73]",
                                     #SUPORT_VEN == 1
                                     )
nrow(x)
a <- filter(x, EVOLUCAO == 2)
nrow(a)

prb <- nrow(a)/nrow(x)
prb

prob.uti.1/prb
```
```{r}
set.seed(42)
dd <- ddf
nrow(dd)
# 76666
ii <- sample(1:76666, 76666, replace=FALSE)
dd$ii <- ii

ii <- sample(1:76000, 76000, replace = FALSE)
amostra <- dd[ii, ]

rede <- mmhc(amostra)

```

```{r echo=FALSE}
boots.trap  <- 300

str.diff = boot.strength(amostra, 
                         R = boots.trap, 
                         algorithm = "mmhc")

rede.media = averaged.network(str.diff)
thr <- paste('Thr: ', attr(str.diff, "threshold"))

strength.plot(rede.media, 
              str.diff, 
              shape = "rectangle", 
              main = paste("Iter = ", boots.trap, thr))
```
```{r}

fitt.1 <- bn.fit(rede.media, amostra)

prob.uti.1 <- cpquery(fitt.1, 
         evidence = list( 
               #'SATURACAO' = "1",
               'IDADE' = "(37,73]" ,
               # (FEBRE == 1) &
               #  (DIABETES == 2) &
               #  (RENAL == 2) &
               "UTI" = "1" ,
               'SUPORT_VEN' = "3" 
               ),
         event =  (EVOLUCAO == 2) ,
         method = "lw")
prob.uti.1

x <- amostra %>% filter(
                                    # SATURACAO == 1, 
                                     UTI == 1, 
                                     #FEBRE == 1, 
                                     #DIABETES == 2, 
                                     #RENAL == 2,
                                     IDADE == "(37,73]",
                                     SUPORT_VEN == 3
                                     )
nrow(x)
a <- filter(x, EVOLUCAO == 2)
nrow(a)

prb <- nrow(a)/nrow(x)
prb

prob.uti.1/prb
```

